{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxPHiy6A0VwS"
      },
      "source": [
        "# Training and Test Errors\n",
        "\n",
        "So far, we have fit regression models to data and obtained predictions from them, but we have not evaluated whether these predictions were any good. Now we will discuss different performance metrics that can be used to evaluate predictions from a machine learning model.\n",
        "\n",
        "To evaluate the performance of a machine learning model, we check the predicted labels from the model against the true labels. A **label** is a value of the target. A true label is denoted $y$ and the predicted label is denoted $\\hat y$.\n",
        "\n",
        "- The data for which we know the label $y$ is called the **training data**.\n",
        "- The data for which we don't know the label $y$ (and want to predict it) is called the **test data**.\n",
        "\n",
        "In a regression model the labels are quantitative so all of the performance metrics are based on the *difference* between each predicted label $\\hat y$ and the true label $y$.\n",
        "\n",
        "We'll use the Bordeaux wine data as example. Recall that our target is wine quality, which we measure as the log of price. The labels are the values of log(price), which we have observed for years up to 1980. That is, the observations up to 1980 comprise the training data. The observations after 1980&mdash;for which log(price) is unknown&mdash;is the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "80PHg6bg0VwW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Extract the training data.\n",
        "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
        "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
        "                          index_col=\"year\")\n",
        "\n",
        "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
        "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isnx6SdJ0VwY"
      },
      "source": [
        "## Performance Metrics for Regression Models\n",
        "\n",
        "One way to measure the performance of a regression model is to compute the difference between each $y$ and $\\hat y$, then square each difference and average the squared differences. This measure of error is known as _mean squared error_ (or _MSE_, for short):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textrm{MSE} &= \\textrm{mean of } (y - \\hat y)^2.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "MSE is difficult to interpret because its units are the square of the units of the label. To make MSE more interpretable, it is common to take the _square root_ of the MSE to obtain the _root mean squared error_ (or _RMSE_, for short):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textrm{RMSE} &= \\sqrt{\\textrm{MSE}}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The RMSE measures how off a \"typical\" prediction is. Notice that this reasoning is exactly the same reasoning that we used earlier when we defined the standard deviation as the square root of the variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_cNobH2zGZ7"
      },
      "source": [
        "Another common measure of error is the _mean absolute error_ (or _MAE_, for short):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\textrm{MAE} &= \\textrm{mean of } |y - \\hat y|.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Like the RMSE, the MAE measures how off a \"typical\" prediction is.\n",
        "\n",
        "MSE, RMSE, and MAE are all error metrics; we want them to be as small as possible. There are also performance metrics that we seek to maximize. One example is $R^2$, also known as the _coefficient of determination_:\n",
        "\n",
        "\\begin{align*}\n",
        "R^2 &= 1 - \\frac{\\text{mean of } (y - \\hat y)^2}{\\text{mean of } (y - \\bar y)^2}.\n",
        "\\end{align*}\n",
        "\n",
        "Notice that the denominator, $\\text{mean of } (y - \\bar y)^2$, is just the variance of the label $y$. So the interpretation of $\\frac{\\text{mean of } (y - \\hat y)^2}{\\text{mean of } (y - \\bar y)^2}$ is the fraction of the variance in the label $y$ that is \"left over\" after we fit the regression model. Therefore, $R^2$ can be interpreted as the fraction of variance that is explained by the regression model. It cannot be greater than $1.0$, but it can sometimes be negative if the regression model is worse than useless.\n",
        "\n",
        "These are just some of the performance metrics that are used to evaluate regression models. For more, refer to the [scikit-learn documentation on regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlNOi900VwZ"
      },
      "source": [
        "## Training Error\n",
        "\n",
        "To calculate the performance metrics above, we need data where the true labels are known. Where do we find such data? One natural source of labeled data is the training data, since we needed the true labels to be able to train a model.\n",
        "\n",
        "For a $k$-nearest neighbors model, the training data is the data from which the $k$-nearest neighbors are selected. So to calculate the training RMSE, we do the following:\n",
        "\n",
        "For each observation in the training data:\n",
        "1. Find its $k$-nearest neighbors in the training data.\n",
        "2. Average the labels of the $k$-nearest neighbors to obtain the predicted label.\n",
        "3. Compare the predicted label to the true label.\n",
        "\n",
        "At this point, we can average the square of these differences to obtain the MSE or average their absolute values to obtain the MAE.\n",
        "\n",
        "Let's calculate the training MSE for the 5-nearest neighbors model based on winter rainfall (**win**) and average summer temperature (**summer**) that we fit in a previous notebook. Don't forget to scale the features in a $k$-nearest neighbors model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "bi2lSYNu0Vwa",
        "outputId": "f46e09f4-5815-44fc-f8be-028ffa984f9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
              "                (&#x27;kneighborsregressor&#x27;, KNeighborsRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
              "                (&#x27;kneighborsregressor&#x27;, KNeighborsRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                ('kneighborsregressor', KNeighborsRegressor())])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X_train = bordeaux_train[[\"win\", \"summer\"]]\n",
        "y_train = bordeaux_train[\"log(price)\"]\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "          StandardScaler(),\n",
        "          KNeighborsRegressor(n_neighbors=5)\n",
        ")\n",
        "\n",
        "pipeline.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmR1H-hX0Vwb"
      },
      "source": [
        "To calculate the training error, we need the model's predictions on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odYvXv60Vwb",
        "outputId": "02309db5-fe46-4755-d39a-1df030acf4d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3.56069882, 3.38240361, 3.66085388, 2.70065469, 2.77198968,\n",
              "       3.4843538 , 3.19493478, 3.76968555, 3.25353221, 2.5331913 ,\n",
              "       3.4843538 , 2.51412926, 3.19493478, 3.08407263, 2.56202526,\n",
              "       2.951578  , 3.32169799, 3.33549657, 2.6527587 , 3.4843538 ,\n",
              "       2.56202526, 3.29634765, 3.4843538 , 2.86435018, 2.86435018,\n",
              "       3.08407263, 2.6316866 ])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the model predictions on the training data.\n",
        "y_train_ = pipeline.predict(X=X_train)\n",
        "y_train_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My8HXTrk0Vwc"
      },
      "source": [
        "Finally, we compare the predictions `y_train_` (note the trailing underscore) to the true labels `y_train`, which are known, since this is the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NiZiYZe0Vwd",
        "outputId": "2ffcb756-4723-4e23-9494-3cf24918f3e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.16338631596342398"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the mean-squared error.\n",
        "mse = ((y_train - y_train_) ** 2).mean()\n",
        "mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oymjKM2x0Vwe"
      },
      "source": [
        "We could have also used a scikit-learn function to calculate mean-squared error. The scikit-learn functions for the performance metrics discussed above are shown in the table below. All of these functions take a 1D-array of the true labels as the first parameter and a 1D-array of the predicted labels as the second.\n",
        "\n",
        "| Metric | Function Name |\n",
        "|--------|---------------|\n",
        "| MSE | `mean_squared_error` |\n",
        "| MAE | `mean_absolute_error` |\n",
        "| $R^2$ | `r2_score` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay-8aWk90Vwe",
        "outputId": "4a90ad94-27b0-47a4-b397-2a0fe756a213"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.16338631596342398"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mean_squared_error(y_train, y_train_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2faJphRR0Vwf"
      },
      "source": [
        "To obtain a measure of error that is more interpretable, we can take the square root to obtain the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-0ZnKzz0Vwf",
        "outputId": "c862759d-108e-46fd-e561-f1d4abcebb78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4042107321230152"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rmse = np.sqrt(mse)\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqGbmh0V0Vwf"
      },
      "source": [
        "The RMSE says that the model's predictions are off by about 0.4 on average. This is not too bad, since vintage quality ranges from 2.0 to 5.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEHrUX9J0Vwg"
      },
      "source": [
        "## Overfitting and Test Error\n",
        "\n",
        "Training error is not a great measure of the quality of a model. To see why, consider a 1-nearest neighbor regression model. Before you read on, can you guess what the training error of a 1-nearest neighbor regression model will be? Pause to think about this question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9btx7lM0Vwg",
        "outputId": "8156a7f7-be47-4182-bb9f-4c89060ed348"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit a 1-nearest neighbors model.\n",
        "model = KNeighborsRegressor(n_neighbors=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the model predictions on the training data.\n",
        "y_train_ = model.predict(X_train)\n",
        "\n",
        "# Calculate the MSE\n",
        "mean_squared_error(y_train, y_train_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPMnsIA10Vwg"
      },
      "source": [
        "The training error of this model seems too good to be true. Can our model really have an error of $0.0$? The reason, of course, is that the nearest neighbor to any observation in the training data is the observation itself!\n",
        "\n",
        "Ultimately, the goal of a machine learning model is to make predictions on *future* data. Therein lies the problem with training error. Training error measures how well a model does on the current data. It is possible to build a model that _overfits_ to the training data&mdash;that is, a model that fits so well to the current data that it does poorly on future data.\n",
        "\n",
        "For example, consider fitting two different models to the 10 training observations shown below. The model represented by the red line actually passes through every observation (that is, its training error is zero). However, most people would prefer the model represented by the blue line. If one had to make a prediction for the label when $x = 0.8$, the value of the blue line is intuitively more plausible than the value of the red line, which is out of step with the nearby points.\n",
        "\n",
        "![](https://github.com/dlsun/pods/blob/master/05-Regression-Models/overfitting.png?raw=1)\n",
        "\n",
        "To make a case for the blue model using mean-squared error, we would need future, or test, data. The red model is as good as it gets when it comes to the training data.\n",
        "\n",
        "The prediction error on future data is also known as _test error_. But to calculate the test error, we need _labeled_ future data. In many applications, data is hard to collect and _labeled_ data is harder still. In the next notebook, we discuss strategies for approximating the test error using only the training data that we aleady have."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
