{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbyscqVK0d3h"
      },
      "source": [
        "# Estimating the Test Error\n",
        "\n",
        "In the previous notebook, we learned about _training error_, which is the error calculated on the training data. Training error is easy to calculate because the true labels are available in the training data. However, training error is not always a good measure of a model's quality, since a model that _overfits_ to the training data may have artificially low training error.\n",
        "\n",
        "Ideally, we would like to evaluate regression models based on their _test error_, which is the error calculated on the test data. The problem with test error is that it is usually impossible to calculate, since the true labels are rarely available in the test data. In this section, we discuss strategies for estimating the test error using only the training data.\n",
        "\n",
        "We'll use the Bordeaux wine data as example. Recall that our target is wine quality, which we measure as the log of price. The labels are the values of log(price), which we have observed for years up to 1980. That is, the observations up to 1980 comprise the training data. The observations after 1980&mdash;for which log(price) is unknown&mdash;is the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tYJ3uRwe0d3l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Extract the training data.\n",
        "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
        "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
        "                          index_col=\"year\")\n",
        "\n",
        "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
        "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBucp7Tw0d3m"
      },
      "source": [
        "## Validation Error\n",
        "\n",
        "To estimate the test error, we split the training data into a _training set_ and a _validation set_. First, the model is fit to just the data in the training set. Then, the model is evaluated based on its predictions on the validation set. Because the model did not train on any of the labels in the validation set, the validation set essentially plays the role of the test data, even though it was carved out of the training data.\n",
        "\n",
        "The prediction error on the validation set is known as the _validation error_. The validation error is an approximation to the test error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwqNgST00d3n"
      },
      "source": [
        "To split our data into training and validation sets, we can use the `.sample()` function in `pandas`. Let's use this to split our data into two equal halves, which we will call `train` and `val`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dad9C0700d3o"
      },
      "outputs": [],
      "source": [
        "train = bordeaux_train.sample(frac=0.5)\n",
        "val = bordeaux_train.drop(train.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyUiCuexWmZJ"
      },
      "source": [
        "Here is the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7-y0Xk8FWmnC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>summer</th>\n",
              "      <th>har</th>\n",
              "      <th>sep</th>\n",
              "      <th>win</th>\n",
              "      <th>age</th>\n",
              "      <th>log(price)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1966</th>\n",
              "      <td>47.0</td>\n",
              "      <td>16.5</td>\n",
              "      <td>86</td>\n",
              "      <td>18.4</td>\n",
              "      <td>819</td>\n",
              "      <td>26</td>\n",
              "      <td>3.850148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>31.0</td>\n",
              "      <td>17.3</td>\n",
              "      <td>96</td>\n",
              "      <td>18.8</td>\n",
              "      <td>402</td>\n",
              "      <td>28</td>\n",
              "      <td>3.433987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965</th>\n",
              "      <td>11.0</td>\n",
              "      <td>15.4</td>\n",
              "      <td>267</td>\n",
              "      <td>14.8</td>\n",
              "      <td>602</td>\n",
              "      <td>27</td>\n",
              "      <td>2.397895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>100.0</td>\n",
              "      <td>17.3</td>\n",
              "      <td>38</td>\n",
              "      <td>20.4</td>\n",
              "      <td>830</td>\n",
              "      <td>31</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1976</th>\n",
              "      <td>25.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>247</td>\n",
              "      <td>16.1</td>\n",
              "      <td>418</td>\n",
              "      <td>16</td>\n",
              "      <td>3.218876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980</th>\n",
              "      <td>14.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>74</td>\n",
              "      <td>18.4</td>\n",
              "      <td>578</td>\n",
              "      <td>12</td>\n",
              "      <td>2.639057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1978</th>\n",
              "      <td>27.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>51</td>\n",
              "      <td>17.4</td>\n",
              "      <td>763</td>\n",
              "      <td>14</td>\n",
              "      <td>3.295837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1957</th>\n",
              "      <td>22.0</td>\n",
              "      <td>16.1</td>\n",
              "      <td>110</td>\n",
              "      <td>16.2</td>\n",
              "      <td>420</td>\n",
              "      <td>35</td>\n",
              "      <td>3.091042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959</th>\n",
              "      <td>66.0</td>\n",
              "      <td>17.5</td>\n",
              "      <td>187</td>\n",
              "      <td>18.7</td>\n",
              "      <td>485</td>\n",
              "      <td>33</td>\n",
              "      <td>4.189655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1979</th>\n",
              "      <td>21.0</td>\n",
              "      <td>16.2</td>\n",
              "      <td>122</td>\n",
              "      <td>17.3</td>\n",
              "      <td>717</td>\n",
              "      <td>13</td>\n",
              "      <td>3.044522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1974</th>\n",
              "      <td>11.0</td>\n",
              "      <td>16.3</td>\n",
              "      <td>184</td>\n",
              "      <td>16.2</td>\n",
              "      <td>574</td>\n",
              "      <td>18</td>\n",
              "      <td>2.397895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1953</th>\n",
              "      <td>63.0</td>\n",
              "      <td>16.7</td>\n",
              "      <td>80</td>\n",
              "      <td>17.3</td>\n",
              "      <td>690</td>\n",
              "      <td>39</td>\n",
              "      <td>4.143135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1968</th>\n",
              "      <td>11.0</td>\n",
              "      <td>16.2</td>\n",
              "      <td>292</td>\n",
              "      <td>16.4</td>\n",
              "      <td>610</td>\n",
              "      <td>24</td>\n",
              "      <td>2.397895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1975</th>\n",
              "      <td>30.0</td>\n",
              "      <td>16.9</td>\n",
              "      <td>171</td>\n",
              "      <td>17.2</td>\n",
              "      <td>572</td>\n",
              "      <td>17</td>\n",
              "      <td>3.401197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      price  summer  har   sep  win  age  log(price)\n",
              "year                                                \n",
              "1966   47.0    16.5   86  18.4  819   26    3.850148\n",
              "1964   31.0    17.3   96  18.8  402   28    3.433987\n",
              "1965   11.0    15.4  267  14.8  602   27    2.397895\n",
              "1961  100.0    17.3   38  20.4  830   31    4.605170\n",
              "1976   25.0    17.6  247  16.1  418   16    3.218876\n",
              "1980   14.0    16.0   74  18.4  578   12    2.639057\n",
              "1978   27.0    15.8   51  17.4  763   14    3.295837\n",
              "1957   22.0    16.1  110  16.2  420   35    3.091042\n",
              "1959   66.0    17.5  187  18.7  485   33    4.189655\n",
              "1979   21.0    16.2  122  17.3  717   13    3.044522\n",
              "1974   11.0    16.3  184  16.2  574   18    2.397895\n",
              "1953   63.0    16.7   80  17.3  690   39    4.143135\n",
              "1968   11.0    16.2  292  16.4  610   24    2.397895\n",
              "1975   30.0    16.9  171  17.2  572   17    3.401197"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSpgvILOWm7w"
      },
      "source": [
        "Here is the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VKTJRgXoWvJW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>summer</th>\n",
              "      <th>har</th>\n",
              "      <th>sep</th>\n",
              "      <th>win</th>\n",
              "      <th>age</th>\n",
              "      <th>log(price)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1952</th>\n",
              "      <td>37.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>160</td>\n",
              "      <td>14.3</td>\n",
              "      <td>600</td>\n",
              "      <td>40</td>\n",
              "      <td>3.610918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1955</th>\n",
              "      <td>45.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>130</td>\n",
              "      <td>16.8</td>\n",
              "      <td>502</td>\n",
              "      <td>37</td>\n",
              "      <td>3.806662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1958</th>\n",
              "      <td>18.0</td>\n",
              "      <td>16.4</td>\n",
              "      <td>187</td>\n",
              "      <td>19.1</td>\n",
              "      <td>582</td>\n",
              "      <td>34</td>\n",
              "      <td>2.890372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>14.0</td>\n",
              "      <td>16.4</td>\n",
              "      <td>290</td>\n",
              "      <td>15.8</td>\n",
              "      <td>763</td>\n",
              "      <td>32</td>\n",
              "      <td>2.639057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1962</th>\n",
              "      <td>33.0</td>\n",
              "      <td>16.3</td>\n",
              "      <td>52</td>\n",
              "      <td>17.2</td>\n",
              "      <td>697</td>\n",
              "      <td>30</td>\n",
              "      <td>3.496508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963</th>\n",
              "      <td>17.0</td>\n",
              "      <td>15.7</td>\n",
              "      <td>155</td>\n",
              "      <td>16.2</td>\n",
              "      <td>608</td>\n",
              "      <td>29</td>\n",
              "      <td>2.833213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1967</th>\n",
              "      <td>19.0</td>\n",
              "      <td>16.2</td>\n",
              "      <td>118</td>\n",
              "      <td>16.5</td>\n",
              "      <td>714</td>\n",
              "      <td>25</td>\n",
              "      <td>2.944439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1969</th>\n",
              "      <td>12.0</td>\n",
              "      <td>16.5</td>\n",
              "      <td>244</td>\n",
              "      <td>16.6</td>\n",
              "      <td>575</td>\n",
              "      <td>23</td>\n",
              "      <td>2.484907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1970</th>\n",
              "      <td>40.0</td>\n",
              "      <td>16.7</td>\n",
              "      <td>89</td>\n",
              "      <td>18.0</td>\n",
              "      <td>622</td>\n",
              "      <td>22</td>\n",
              "      <td>3.688879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1971</th>\n",
              "      <td>27.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>112</td>\n",
              "      <td>16.9</td>\n",
              "      <td>551</td>\n",
              "      <td>21</td>\n",
              "      <td>3.295837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1972</th>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>158</td>\n",
              "      <td>14.6</td>\n",
              "      <td>536</td>\n",
              "      <td>20</td>\n",
              "      <td>2.302585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1973</th>\n",
              "      <td>16.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>123</td>\n",
              "      <td>17.9</td>\n",
              "      <td>376</td>\n",
              "      <td>19</td>\n",
              "      <td>2.772589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1977</th>\n",
              "      <td>11.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>87</td>\n",
              "      <td>16.8</td>\n",
              "      <td>821</td>\n",
              "      <td>15</td>\n",
              "      <td>2.397895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      price  summer  har   sep  win  age  log(price)\n",
              "year                                                \n",
              "1952   37.0    17.1  160  14.3  600   40    3.610918\n",
              "1955   45.0    17.1  130  16.8  502   37    3.806662\n",
              "1958   18.0    16.4  187  19.1  582   34    2.890372\n",
              "1960   14.0    16.4  290  15.8  763   32    2.639057\n",
              "1962   33.0    16.3   52  17.2  697   30    3.496508\n",
              "1963   17.0    15.7  155  16.2  608   29    2.833213\n",
              "1967   19.0    16.2  118  16.5  714   25    2.944439\n",
              "1969   12.0    16.5  244  16.6  575   23    2.484907\n",
              "1970   40.0    16.7   89  18.0  622   22    3.688879\n",
              "1971   27.0    16.8  112  16.9  551   21    3.295837\n",
              "1972   10.0    15.0  158  14.6  536   20    2.302585\n",
              "1973   16.0    17.1  123  17.9  376   19    2.772589\n",
              "1977   11.0    15.6   87  16.8  821   15    2.397895"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxH8zzxH0d3o"
      },
      "source": [
        "Now let's use these training/validation sets to approximate the test MSE of the 5-nearest neighbors model, based on **win** and **summer**, that we have fit previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iLQJdv820d3q"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
              "                (&#x27;kneighborsregressor&#x27;, KNeighborsRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
              "                (&#x27;kneighborsregressor&#x27;, KNeighborsRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                ('kneighborsregressor', KNeighborsRegressor())])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# extract features and label from training set\n",
        "X_train = train[[\"win\", \"summer\"]]\n",
        "y_train = train[\"log(price)\"]\n",
        "\n",
        "# define pipeline and fit to training set\n",
        "pipeline = make_pipeline(\n",
        "          StandardScaler(),\n",
        "          KNeighborsRegressor(n_neighbors=5)\n",
        ")\n",
        "pipeline.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teshHVeD0d3r"
      },
      "source": [
        "We make predictions on the validation set and calculate the validation RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7XlBjg3Z0d3s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4650790869716535"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# extract features and label from validation set\n",
        "X_val = val[[\"win\", \"summer\"]]\n",
        "y_val = val[\"log(price)\"]\n",
        "\n",
        "# get model's predictions on validation set\n",
        "y_val_ = pipeline.predict(X_val)\n",
        "\n",
        "# calculate RMSE on validation set\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_))\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peZAc7N80d3s"
      },
      "source": [
        "Notice that the test error is higher than the training error that we calculated in the previous lesson. In general, this will be true. It is harder for a model to predict for new observations it has not seen, than for observations it has seen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6AOb390d3t"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "The validation error above was calculated using only 50% of the training data, since we split the training data in half to create the validation set. As a result, the estimate is noisy.\n",
        "\n",
        "There is a cheap way to obtain a second opinion about how well our model will do on future data. Previously, we split our data at random into two halves, fitting the model to the first half and evaluating it on the second half. Because the model has not already seen the second half of the data, this approximates how well the model would perform on future data.\n",
        "\n",
        "But the way we split our data was arbitrary. We might as well swap the roles of the two halves, fitting the model to the _second_ half and evaluating it on the _first_ half. As long as the model is always evaluated on data that is different from the data that was used to train it, we have a valid estimate of test error. A schematic of this approach, known as **cross-validation**, is shown below.\n",
        "\n",
        "![](https://github.com/dlsun/pods/blob/master/05-Regression-Models/cross-validation.png?raw=1)\n",
        "\n",
        "Because we will be doing all computations twice, just with different data, let's wrap the $k$-nearest neighbors algorithm above into a function called `get_val_error()`, that computes the validation error given training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yvs4ACVg0d3t"
      },
      "outputs": [],
      "source": [
        "def get_val_error(train, val):\n",
        "\n",
        "    # extract features and label from training set.\n",
        "    X_train = train[[\"win\", \"summer\"]]\n",
        "    y_train = train[\"log(price)\"]\n",
        "\n",
        "    # define pipeline and fit to training set\n",
        "    pipeline = make_pipeline(\n",
        "          StandardScaler(),\n",
        "          KNeighborsRegressor(n_neighbors=5)\n",
        "    )\n",
        "    pipeline.fit(X=X_train, y=y_train)\n",
        "\n",
        "    # extract features and label from validation set\n",
        "    X_val = val[[\"win\", \"summer\"]]\n",
        "    y_val = val[\"log(price)\"]\n",
        "\n",
        "    # get model's predictions on validation set\n",
        "    y_val_ = pipeline.predict(X_val)\n",
        "\n",
        "    # calculate RMSE on validation set\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_val_))\n",
        "\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHPwSya00d3u"
      },
      "source": [
        "If we apply this function to the training and validation sets from earlier, we get the same estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bi-0nPju0d3u"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4650790869716535"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_val_error(train, val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG2TJc1V0d3v"
      },
      "source": [
        "But if we reverse the roles of the training and validation sets, we get another estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b1hlgxPN0d3v"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6167043190959203"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_val_error(val, train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14IV2PCQ0d3v"
      },
      "source": [
        "Now we have two, somewhat independent estimates of the test error, which could be quite different. It is common to average the two numbers to obtain an overall estimate of the test error, called the **cross-validation estimate of test error**. Notice that the cross-validation estimate uses each observation in the data exactly once. We make a prediction for each observation, but always using a model that was trained on data that does not include that observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3itWyZkjXBUf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5408917030337869"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(get_val_error(train, val) + get_val_error(val, train)) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ZBKkUe0d3w"
      },
      "source": [
        "## Cross-Validation in scikit-learn\n",
        "\n",
        "As you know by now, scikit-learn provides functions that automate routine tasks of machine learning. For cross-validation, there is a function, `cross_val_score`, that takes in a model (or pipeline), the training data, and a scoring function, and carries out all aspects of cross-validation, including\n",
        "\n",
        "1. splitting the training data into training and validation sets\n",
        "2. fitting the model to each training set\n",
        "3. calculating the model's predictions on the corresponding validation set\n",
        "4. calculating the score of the predictions on each validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZyxijRIC0d3w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.47767636, -0.38655855])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(pipeline,\n",
        "                         X=bordeaux_train[[\"win\", \"summer\"]],\n",
        "                         y=bordeaux_train[\"log(price)\"],\n",
        "                         scoring=\"neg_mean_squared_error\",\n",
        "                         cv=2)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uowj7sFF0d3w"
      },
      "source": [
        "First, notice that there are 2 scores. This is because scikit-learn calculated a score from each half of the data when that half served as the validation set.\n",
        "\n",
        "Second, observe that the scores are negative. This is because scikit-learn requires that a \"score\" be something that ought to be maximized. Since we want to minimize the mean-squared error, we want to maximize the *negative* mean-squared error. Therefore, the scores that are reported here are the negative of the MSE.\n",
        "\n",
        "To calculate the RMSE, we negate the negative sign and take a square root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TkByoFcU0d3w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.69114134, 0.62173833])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(-scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Wqf1Sk0d3x"
      },
      "source": [
        "The average of these two scores is the cross-validation estimate of test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FfjLJgG20d3x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6564398373317621"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(-scores).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBRVrAjYYH7V"
      },
      "source": [
        "Note that this is not the same estimate of test error we got from scratch. The difference is due to the random splitting of the data into training and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBT-EBpv0d3x"
      },
      "source": [
        "## $K$-Fold Cross Validation\n",
        "\n",
        "One problem with splitting the training data into two halves is that the model is now trained on only half the amount of data. This model will likely perform worse than the actual model, which is trained on all of the training data. So the cross-validation estimate of test error is unnecessarily pessimistic.\n",
        "\n",
        "We would like the size of the training set to be closer to the size of the original training data. We can do this by splitting the data into more than two subsamples. In general, we can split the data into $K$ subsamples, alternately training the data on $K-1$ subsamples and evaluating the model on the $1$ remaining subsample, i.e., the validation set. This way, we use $100(1 − 1/K)$ percent of the data for training in each of the \"folds\". This produces $K$ somewhat independent estimates of the test error. This procedure is known as **$K$-fold cross validation**. (Be careful not to confuse this $K$ with the $k$ in $k$-nearest neighbors.) In hindsight, the  cross validation that we were doing previously is $2$-fold cross validation.\n",
        "\n",
        "A schematic of $4$-fold cross validation is shown below.\n",
        "\n",
        "![](https://github.com/dlsun/pods/blob/master/05-Regression-Models/k-folds.png?raw=1)\n",
        "\n",
        "Implementing $K$-fold cross validation in scikit-learn is easy. We simply set the `cv=` parameter to the desired number of folds. For example, the following code carries out $4$-fold cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XaSTsmOk0d3y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.47213584, -0.30601689, -0.36874833, -0.11290777])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = cross_val_score(pipeline,\n",
        "                         X=bordeaux_train[[\"win\", \"summer\"]],\n",
        "                         y=bordeaux_train[\"log(price)\"],\n",
        "                         scoring=\"neg_mean_squared_error\",\n",
        "                         cv=4)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSIMCGs80d3y"
      },
      "source": [
        "Notice that $K$ scores are produced&mdash;4 in this case&mdash;one from each fold. These scores can be averaged to produce a single estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oKhf7kio0d3z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.545893343925109"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(-scores).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrRJZo-zyhiG"
      },
      "source": [
        "How many folds to use in cross validation? In practice, 5-fold and 10-fold cross-validation are commonly used, and this is primarily what we'll do.\n",
        "\n",
        "Another common choice is to set $K$ equal to $n$, the number of observations in the training data, resulting in **leave one out (LOO)** cross validation.\n",
        "\n",
        "When choosing the number of folds there is a bias-variability tradeoff. Fewer folds introduces more bias, since in each fold only a subset of the data is used to fit the model (e.g., only half the data when $K=2$), and models fit on less data tend to perform less well and tend to overestimate the test error rate. With many folds&mdash;such as LOO at the extreme&mdash;there is less bias in estimating the test error because the training set within each fold is roughly the same size as the training data.\n",
        "\n",
        "\n",
        "However, more folds leads to more variability in estimates of test error. With many folds, the training set is roughly the same across the folds resulting in estimates of test error that are highly positively correlated. With fewer folds, there is less overlap between training sets across the folds, therefore resulting in $K$ roughly independent measurements of test error. The mean of positively correlated quantities has more variability that the mean of independent quantities (remember, $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)$), and so the more folds the more variability.\n",
        "\n",
        "In short, increasing the number of folds decreases bias but increases variability. We won't explore these issues further. Instead, we'll just rely on the common practice of using 5 or 10 folds."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
