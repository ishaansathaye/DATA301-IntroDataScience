{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Distances Between Observations"
      ],
      "metadata": {
        "id": "S5UO8GoEF3W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2_pNolhpF_2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ames housing - three variables only\n",
        "\n",
        "As in the reading, first we will work with just three quantitative variables from that data set: the number of bedrooms, the number of bathrooms, and the living area (in square feet)."
      ],
      "metadata": {
        "id": "oa7-J4XwGEgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing = pd.read_csv(\"https://raw.githubusercontent.com/kevindavisross/data301/main/data/AmesHousing.txt\", sep=\"\\t\")\n",
        "df_housing[\"Bathrooms\"] = df_housing[\"Full Bath\"] + 0.5 * df_housing[\"Half Bath\"]\n",
        "df_housing_quant = df_housing[[\"Bedroom AbvGr\", \"Gr Liv Area\", \"Bathrooms\"]]\n",
        "df_housing_quant"
      ],
      "metadata": {
        "id": "Pc8W5LVuGAed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the reading, we scaled these variables using standardized scaling, then computed the Euclidean distance between observations 2927 and 2498 and between observations 2928 and 290."
      ],
      "metadata": {
        "id": "lK5DZqzuGyd5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPJouSPfkRIf"
      },
      "source": [
        "1\\. Instead of standardizing the three variables from the Ames housing data set, normalize them.\n",
        "\n",
        "You should do this from scratch, without using scikit-learn. (You can also try scikit-learn, but remember that the `Normalizer` scaler normalizes the rows to be length 1, rather than the columns. The scikit-learn function `normalize` is simpler, and allows you to normalize rows or columns using the `axis` argument.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "GmNbF5t8HWng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2\\. Recompute the Euclidean distances between the two pairs of points, but using the normalized values."
      ],
      "metadata": {
        "id": "EuhhOIDMHW1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "dTMb2uVeH4h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT5rNT3AkRIm"
      },
      "source": [
        "3\\. Instead of standardizing the three variables from the Ames housing data set, apply a min-max scaling to them.\n",
        "\n",
        "Try this both from scratch and using the `MinMaxScaler` in scikitlearn."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "VSaa6sEwH_nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4\\. Recompute the Euclidean distances between the two pairs of points, but using the min-max-scaled values."
      ],
      "metadata": {
        "id": "4Ik00tnnH_0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "fs9B_GdNIEYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5\\. Does your conclusion about which pair of observations is most similar change depending on the scaling you use?"
      ],
      "metadata": {
        "id": "Ys2nS9wZIElg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR RESPONSE HERE**"
      ],
      "metadata": {
        "id": "ZkZ8wQdgIQrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6\\. Suppose that you really like house 0 in the data set, but it is too expensive. Find cheaper homes that are similar to it --- in terms of living area, number of bedrooms, number of bathrooms --- by calculating distances from house 0. Try different distance metrics and different scaling methods. How sensitive are your results to these choices?\n",
        "\n",
        "Be sure to actually look at the profiles of the homes that your algorithm picked out as most similar. Do they make sense?\n",
        "\n",
        "_Think:_ If the goal is to find a \"good deal\" on a similar house, should sale price be included as a variable in your distance metric?"
      ],
      "metadata": {
        "id": "_kpDn48RJFkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "SfFKx37MJFvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR RESPONSE HERE**"
      ],
      "metadata": {
        "id": "iviIb106YIfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using categorical variables when computing distances\n",
        "\n",
        "So far, we have only computed distances between observations based on quantitative variables. But what if we want to include categorical variables? We can convert categorical variables into dummy quantitative variables, and then include in the dummy variables in the distance calculations.\n",
        "\n",
        "Let's add \"House Style\" to the variables we are considering for the Ames housing data set.\n"
      ],
      "metadata": {
        "id": "EZgWttTbJwnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing_mixed = df_housing[[\"Bedroom AbvGr\", \"Gr Liv Area\", \"Bathrooms\", \"House Style\"]]\n",
        "df_housing_mixed"
      ],
      "metadata": {
        "id": "z301TRZ2K079"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we have seen the Pandas `get_dummies()` command which converts all categorical variables into dummy variables (leaving quantitative variables as is)."
      ],
      "metadata": {
        "id": "V9-iRfPDLYAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_housing_dummies = pd.get_dummies(df_housing_mixed)\n",
        "df_housing_dummies"
      ],
      "metadata": {
        "id": "5PGQ37y-LYY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7\\. Continuing part 6. Suppose that you really like house 0 in the data set, but it is too expensive. Find cheaper homes that are similar to it --- in terms of living area, number of bedrooms, number of bathrooms, **and House Style** --- by calculating distances from house 0. Try different distance metrics and different scaling methods. How sensitive are your results to these choices?\n",
        "\n",
        "Be sure to actually look at the profiles of the homes that your algorithm picked out as most similar. Do they make sense?\n",
        "\n",
        "_Think:_ If the goal is to find a \"good deal\" on a similar house, should sale price be included as a variable in your distance metric?"
      ],
      "metadata": {
        "id": "qIRCB4erWBJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "cWIjV0v7WMuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR RESPONSE HERE**"
      ],
      "metadata": {
        "id": "FyBtg8VhYMMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity\n",
        "\n",
        "Continuing parts 6 and 7. Suppose that you really like house 0 in the data set, but it is too expensive. Find cheaper homes that are similar to it, by calculating distances after encoding categorical variables as dummy variables. Be sure to actually look at the profiles of the homes that your algorithm picked out as most similar. Do they make sense?\n",
        "\n",
        "Try different distance metrics and different scaling methods. How sensitive are your results to these choices?\n",
        "\n",
        "_Think:_ If the goal is to find a \"good deal\" on a similar house, should sale price be included as a variable in your distance metric?\n",
        "\n",
        "_Hint:_ There are too many variables in the data set. Do not attempt to call `pd.get_dummies()` on the entire `DataFrame`! You will want to pare down the number of variables, but be sure to include a mixture of categorical and quantitative variables. Refer to the [data documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt) for information about the variables.\n",
        "\n",
        "There are many approaches to this problem. I'll ask several groups to present their approach. Which variables did you decide to include? Which scaling method? Which distance matric? Why? What houses would you recommend?"
      ],
      "metadata": {
        "id": "SD8DvV6kVhHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE. ADD CELLS AS NEEDED"
      ],
      "metadata": {
        "id": "66HacMRbWu3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR RESPONSE HERE**"
      ],
      "metadata": {
        "id": "hg7Okcr4VhPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy encoding in scikit-learn and sparse matrices\n",
        "\n",
        "You can do dummy, or \"onehot\", encoding in scikit-learn using `OneHotEncoder`. There are `fit` and `transform` steps, just like for `StandardScaler`."
      ],
      "metadata": {
        "id": "N1MRM2_IJwqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(df_housing[[\"House Style\"]])\n",
        "output = enc.transform(df_housing[[\"House Style\"]])\n",
        "output\n"
      ],
      "metadata": {
        "id": "xHeUylkMMEsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that `OneHotEncoder` returns a \"sparse matrix\", which is not a `DataFrame` or even a `numpy` array. A _sparse matrix_ is one whose entries are mostly zeroes. For example,\n",
        "\n",
        "$$ \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 1.7 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -0.8 & 0 \\end{pmatrix} $$\n",
        "\n",
        "is an example of a sparse matrix. Instead of storing 20 values (most of which are equal to 0), we can simply store the locations of the non-zero entries and their values:\n",
        "\n",
        "- $(1, 0) \\rightarrow 1.7$\n",
        "- $(3, 3) \\rightarrow -0.8$\n",
        "\n",
        "All other entries of the matrix are assumed to be zero. This representation offers substantial memory savings when there are only a few non-zero entries. (But if not, then this representation can actually be more expensive.) Transforming a categorical variable into dummy variables usually returns a sparse matrix, since each row only has one non-zero entry."
      ],
      "metadata": {
        "id": "v_oCsmYUMMxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want a dense matrix instead of a sparse matrix, set `sparse_output=False` in `OneHotEncoder`.\n"
      ],
      "metadata": {
        "id": "0yxwTI6JNbTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "enc.fit(df_housing[[\"House Style\"]])\n",
        "enc.transform(df_housing[[\"House Style\"]])\n"
      ],
      "metadata": {
        "id": "PxCMEw6zNcpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also convert a sparse matrix to dense using `.todense()`"
      ],
      "metadata": {
        "id": "fzoTZOfbNuPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.todense()"
      ],
      "metadata": {
        "id": "xuNtf4O8N06J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selectively Encoding Variables in Scikit-Learn\n",
        "\n",
        "What if we have a DataFrame, and we only want to dummy encode the categorical variables? We have seen that Pandas `get_dummies` will pass through the quantitative variables unchanged. What about scikit-learn? Scikit-learn provides a `ColumnTransformer` that allows us to selectively apply transformations to certain columns. We can use `ColumnTransformer` to apply the `OneHotEncoder` to the \"House Style\" variable, and \"passthrough\" the remaining variables.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5lLxHunFJwst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "enc = ColumnTransformer(\n",
        "    [(\"Encoded House Style\", OneHotEncoder(), [\"House Style\"])],\n",
        "    remainder=\"passthrough\")\n",
        "\n",
        "enc.fit(df_housing_mixed)\n",
        "enc.transform(df_housing_mixed)\n"
      ],
      "metadata": {
        "id": "Dt2BruxlOXaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One advantage of using `ColumnTransformer` is that you can mix scalers for quantitative variables and encoders for categorical variables.\n",
        "\n",
        "(Note: We will see later how to combine steps like these into a pipeline which both streamlines our analysis and allows us to apply operations consistently across multiple data sets, for example, across both training and testing data.)"
      ],
      "metadata": {
        "id": "QXPzDo8jPIu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "enc = ColumnTransformer(\n",
        "    [(\"Scaled Quant Variables\", StandardScaler(), [\"Bedroom AbvGr\", \"Gr Liv Area\", \"Bathrooms\"]),\n",
        "     (\"Encoded House Style\", OneHotEncoder(), [\"House Style\"])],\n",
        "    remainder=\"passthrough\")"
      ],
      "metadata": {
        "id": "jkFRf-PxPNay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the steps in the ColumnTransformer"
      ],
      "metadata": {
        "id": "vZhvmC8JQ3pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc"
      ],
      "metadata": {
        "id": "QRM4CPBaQ3zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we fit the column transformer to the entire Ames housing data set. Notice that variables we haven't specified will passthrough unchanged"
      ],
      "metadata": {
        "id": "Xyof2g3jQ39D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc.fit(df_housing)\n",
        "df_housing_enc = enc.transform(df_housing)\n",
        "\n",
        "df_housing_enc"
      ],
      "metadata": {
        "id": "OkENfj-kQzEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert the output to a Pandas DataFrame, but unforunately, all of the column names have been stripped away."
      ],
      "metadata": {
        "id": "1Mds5-tATsYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(df_housing_enc)"
      ],
      "metadata": {
        "id": "VHYDYAfIR8pL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}